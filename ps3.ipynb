{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers accelerate faiss-gpu pymupdf --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9680fb5039b41ba8aeef90cde7cf23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Mistral-7B for enrichment\n",
    "'''mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")'''\n",
    "\n",
    "\n",
    "# Load Zephyr-7B with full chat capabilities\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    doc.close()\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_with_regex(text: str) -> dict:\n",
    "    def extract_field(field_name: str) -> str:\n",
    "        # Matches `field_name:` and captures everything until the next label or end of string\n",
    "        pattern = rf\"{field_name}\\s*:\\s*(.*?)(?=\\n(?:\\w+\\s*:)|\\Z)\"\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        return match.group(1).strip() if match else \"\"\n",
    "\n",
    "    return {\n",
    "        \"policy_type\": extract_field(\"policy_type\"),\n",
    "        \"coverage\": extract_field(\"coverage\"),\n",
    "        \"content\": extract_field(\"content\")\n",
    "    }\n",
    "def enrich_chunk_with_zephyr(section_text: str, section_title: str, source: str) -> dict:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an AI assistant helping extract structured information from insurance policy documents. \"\n",
    "                \"Your job is to return a valid JSON object with the following fields:\\n\\n\"\n",
    "                \"- section_title: The title of the section (same as input)\\n\"\n",
    "                \"- content: A cleaned, complete, and meaningful paragraph in natural language summarizing the key information from the section. \"\n",
    "                \"This should be plain text — not a dictionary or nested structure. Think like a human explaining this section in full sentences.\\n\"\n",
    "                \"- policy_type: Extract only if clearly mentioned (e.g., Health, Life, or product name like my:health Suraksha)\\n\"\n",
    "                \"- coverage: Only if benefits, conditions, or limits are clearly described\\n\\n\"\n",
    "                \"If any field is not present, leave it as an empty string. \"\n",
    "                \"Return ONLY a valid JSON object. No extra markdown, explanation, or formatting.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Section Title: {section_title}\\nContent:\\n{section_text}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = pipe(prompt, max_new_tokens=512, do_sample=False)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "        # Extract the JSON portion only\n",
    "        json_part = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "        print(json_part)\n",
    "\n",
    "        try:\n",
    "            metadata = json.loads(json_part)\n",
    "            metadata[\"source\"] = source\n",
    "            metadata.setdefault(\"section_title\", section_title)\n",
    "            metadata.setdefault(\"content\", section_text)\n",
    "            metadata[\"error\"] = None\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"⚠️ JSON parsing failed, attempting regex fallback...\")\n",
    "            regex_data = extract_with_regex(section_text)\n",
    "            print(regex_data)\n",
    "            if regex_data[\"policy_type\"] or regex_data[\"coverage\"]:\n",
    "                metadata = {\n",
    "                    \"source\": source,\n",
    "                    \"section_title\": section_title,\n",
    "                    \"content\": regex_data[\"content\"],\n",
    "                    \"policy_type\": regex_data[\"policy_type\"],\n",
    "                    \"coverage\": regex_data[\"coverage\"],\n",
    "                    \"error\": \"LLM parse failed - regex fallback used\"\n",
    "                }\n",
    "            else:\n",
    "                print(\"❌ Both LLM JSON and regex extraction failed.\")\n",
    "                metadata = {\n",
    "                    \"source\": source,\n",
    "                    \"section_title\": section_title,\n",
    "                    \"content\": section_text,\n",
    "                    \"policy_type\": \"\",\n",
    "                    \"coverage\": \"\",\n",
    "                    \"error\": \"LLM parse failed & regex fallback both failed\"\n",
    "                }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"enrich_chunk_with_zephyr failed for section '{section_title}': {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"Section Title: {metadata['section_title']}\\n{metadata['content']}\",\n",
    "        \"metadata\": metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrich_chunk_with_zephyr(\"\"\"my: health Suraksha General Conditions Proposer • Minimum Entry Age - 18 Years • Maximum Enty Age - Lifetime Entry Adult Dependent • Minimum Entry Age - 18 Years • Maximum Entry Age - Lifetime Entry Child/Children • Minimum Entry Age - 91 Days • Maximum Entry Age - 25 Years 1. Entry Age: my:health Suraksha, and Unlimited Retore (Add on) 2. Type of Policy:  The base policy can be issued on individual, multi-individual and family ﬂoater basis  In case of Family Floater policies ﬂoater discount of 50% will be applied on all the members except the oldest member 3. Coverage for dependents  Individual Sum Insured Option: • Proposer • Dependent children • Grandmother • Grandson • Daughter-in-law • Sister • Sister-in-law • Niece • Spouse • Dependant parents/in-laws • Grandfather • Granddaughter • Son-in-law • Brother • Nephew • Brother-in-law Floater sum insured option: Self, spouse, dependent children* and dependent parents/parents in law can be covered under ﬂoater option *Dependent children: A child is considered a dependent for insurance purposes until his 25th birthday provided he is ﬁnancially dependent, on the proposer. 4. Policy period: This policy can be issued for 1 year/ 2 years/ 3 years.\"\"\",\"Paragraph 7\",\"hdfc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_paragraphs(text: str, min_length: int = 100) -> List[Dict]:\n",
    "    raw_chunks = re.split(r'\\n{2,}', text)\n",
    "    sections = []\n",
    "    for i, chunk in enumerate(raw_chunks):\n",
    "        # Remove line breaks and tabs, normalize spacing\n",
    "        cleaned = re.sub(r'[\\n\\t\\r]+', ' ', chunk)\n",
    "        cleaned = re.sub(r'\\s{2,}', ' ', cleaned).strip()\n",
    "\n",
    "        if len(cleaned) >= min_length:\n",
    "            sections.append({\n",
    "                \"section_title\": f\"Paragraph {i+1}\",\n",
    "                \"content\": cleaned\n",
    "            })\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"./policy_pdfs\"\n",
    "output_json_dir = \"./enriched_jsons\"\n",
    "os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "print(f\"📄 Found {len(pdf_files)} PDFs.\")\n",
    "\n",
    "all_enriched = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    full_path = os.path.join(pdf_dir, pdf_file)\n",
    "    print(f\"🧾 Processing {pdf_file}...\")\n",
    "    \n",
    "    # Extract and chunk\n",
    "    text = extract_text_from_pdf(full_path)\n",
    "    chunks = chunk_text_by_paragraphs(text)\n",
    "\n",
    "    enriched = []\n",
    "    for chunk in chunks:\n",
    "        enriched_chunk = enrich_chunk_with_zephyr(\n",
    "            section_text=chunk[\"content\"],\n",
    "            section_title=chunk[\"section_title\"],\n",
    "            source=pdf_file\n",
    "        )\n",
    "        enriched.append(enriched_chunk)\n",
    "\n",
    "    # Save per file\n",
    "    json_path = os.path.join(output_json_dir, pdf_file.replace(\".pdf\", \"_enriched.json\"))\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    all_enriched.extend(enriched)\n",
    "    print(f\"✅ Extracted and enriched {len(enriched)} chunks → {json_path}\")\n",
    "\n",
    "# Build FAISS index from all enriched chunks\n",
    "#build_faiss_index(all_enriched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Model loaded!\n",
      "📁 Found 1 JSON file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding JSONs: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated 11 embeddings with cleaned metadata (no 'error' field).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Load model\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "print(\"🚀 Model loaded!\")\n",
    "\n",
    "# ✅ Directory paths\n",
    "json_folder = \"./enriched_jsons\"\n",
    "\n",
    "# ✅ Load and embed texts\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "\n",
    "json_files = [f for f in os.listdir(json_folder) if f.endswith(\"_enriched.json\")]\n",
    "print(f\"📁 Found {len(json_files)} JSON file(s)\")\n",
    "\n",
    "for file in tqdm(json_files, desc=\"Embedding JSONs\"):\n",
    "    path = os.path.join(json_folder, file)\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data:\n",
    "        text = entry[\"text\"]\n",
    "        metadata = entry[\"metadata\"]\n",
    "\n",
    "        # ✅ Remove \"error\" field if it exists\n",
    "        metadata.pop(\"error\", None)\n",
    "\n",
    "        # Generate embedding\n",
    "        embedding = model.encode(text, show_progress_bar=False)\n",
    "\n",
    "        all_embeddings.append(embedding)\n",
    "        all_metadata.append(metadata)\n",
    "\n",
    "print(f\"\\n✅ Generated {len(all_embeddings)} embeddings with cleaned metadata (no 'error' field).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 FAISS index created and 11 vectors added.\n",
      "✅ FAISS index and metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Install FAISS\n",
    "!pip install faiss-gpu --quiet  # Use faiss-gpu if you're on a GPU runtime\n",
    "\n",
    "# ✅ Imports\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ✅ Convert embeddings list to NumPy array\n",
    "embedding_dim = len(all_embeddings[0])\n",
    "embedding_matrix = np.array(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# ✅ Create FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance (Euclidean)\n",
    "index.add(embedding_matrix)\n",
    "print(f\"📦 FAISS index created and {index.ntotal} vectors added.\")\n",
    "\n",
    "# ✅ Save the index and metadata\n",
    "faiss.write_index(index, \"faiss_index_bge_base.index\")\n",
    "\n",
    "# Save metadata using pickle\n",
    "with open(\"faiss_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_metadata, f)\n",
    "\n",
    "print(\"✅ FAISS index and metadata saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
